[project]
name = "backend"
version = "0.1.0"
description = "Add your description here"
requires-python = ">=3.13,<3.14"
dependencies = [
    "aiohttp>=3.13.2",
    "alembic>=1.17.2",
    "asyncpg>=0.30.0",
    "fastapi-users[oauth,sqlalchemy]>=15.0.1",
    "fastapi[standard]>=0.121.3",
    "httpx-oauth>=0.16.1",
    "loguru>=0.7.3",
    "pandas>=2.3.3",
    "pydantic-settings>=2.12.0",
    "python-multipart>=0.0.20",
    "sqlalchemy>=2.0.44",
    "tqdm>=4.67.1",
    "openai-whisper>=20231117",
    "torch>=2.0.0",
    "sentence-transformers>=5.2.0",
    "pydantic>=2.12.4",
    "huggingface-hub>=0.36.0",
    "rank-bm25>=0.2.2",
    "llama-cpp-python>=0.3.16",
    "numba>=0.60.0",
    "pgvector>=0.3.6",
    "google-genai>=0.3.0",
]

[[tool.uv.index]]
name = "pypi"
url = "https://pypi.org/simple"
default = true

[[tool.uv.index]]
name = "pytorch"
url = "https://download.pytorch.org/whl/cu124"
explicit = true

[tool.uv.sources]
torch = [
  { index = "pytorch", marker = "sys_platform == 'linux'" },
  { index = "pytorch", marker = "sys_platform == 'win32'" },
]
torchvision = [
  { index = "pytorch", marker = "sys_platform == 'linux'" },
  { index = "pytorch", marker = "sys_platform == 'win32'" },
]
torchaudio = [
  { index = "pytorch", marker = "sys_platform == 'linux'" },
  { index = "pytorch", marker = "sys_platform == 'win32'" },
]
llama-cpp-python = [
  { path = "local_wheels/llama_cpp_python-0.3.16-cp313-cp313-win_amd64.whl", marker = "sys_platform == 'win32'" },
  { index = "pypi", marker = "sys_platform != 'win32'" }
]

[dependency-groups]
dev = [
    "openpyxl>=3.1.5",
    "pytest>=9.0.2",
    "pytest-asyncio>=0.25.0",
    "pytest-cov>=7.0.0",
    "pytest-html>=4.2.0",
]

[tool.ruff]
exclude = [
    "alembic",
]
